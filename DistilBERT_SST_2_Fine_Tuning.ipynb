{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIxivNHwTKKkjVfVW7VO8S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fukagai-takuya/gifu-ai/blob/main/DistilBERT_SST_2_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. fine-tuning した DistilBERT のモデル等を保存するため Google Drive をマウントします。"
      ],
      "metadata": {
        "id": "iNdRBvLek9ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg32auz-jEcH",
        "outputId": "836a0c23-bc2f-42e8-a826-644810e3d4eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. WandB を無効にします。\n",
        "- WandB を使用する場合はこのコードセルは実行しないようにします。使用する場合は後のコードセルで fine-tuning を実行する際に WandB の API キーをセットする必要があります。"
      ],
      "metadata": {
        "id": "_b2-PKFHlaON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\""
      ],
      "metadata": {
        "id": "hRubPNN7zScG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. SST-2 のデータセットを読み込みます。\n",
        "- SST-2 のデータセットは映画レビューの文章が positive か negative かに分類したデータセットです。\n",
        "- train, validation データセットを pandas のライブラリで読み込みます。\n",
        "- test データセットの label は全て -1 となっているため、このノートブックでは train データセットを training 用と test 用のデータに分割して使用します。"
      ],
      "metadata": {
        "id": "_rgvWPEu17ES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "splits = {'train': 'data/train-00000-of-00001.parquet', 'validation': 'data/validation-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
        "df_train = pd.read_parquet(\"hf://datasets/stanfordnlp/sst2/\" + splits[\"train\"])\n",
        "df_validation = pd.read_parquet(\"hf://datasets/stanfordnlp/sst2/\" + splits[\"validation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bexjZNLg0StA",
        "outputId": "21d66191-479b-4e30-9863-3a06e3f9c791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1ul5RiE09CG",
        "outputId": "4556a956-05c8-405d-8a83-db2bb92bb652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         idx                                           sentence  label\n",
            "0          0       hide new secretions from the parental units       0\n",
            "1          1               contains no wit , only labored gags       0\n",
            "2          2  that loves its characters and communicates som...      1\n",
            "3          3  remains utterly satisfied to remain the same t...      0\n",
            "4          4  on the worst revenge-of-the-nerds clichés the ...      0\n",
            "...      ...                                                ...    ...\n",
            "67344  67344                               a delightful comedy       1\n",
            "67345  67345                   anguish , anger and frustration       0\n",
            "67346  67346  at achieving the modest , crowd-pleasing goals...      1\n",
            "67347  67347                                  a patient viewer       1\n",
            "67348  67348  this new jangle of noise , mayhem and stupidit...      0\n",
            "\n",
            "[67349 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_validation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QUhG-qT75nt",
        "outputId": "34ae00d8-39ef-4ce3-a60d-470321a950d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     idx                                           sentence  label\n",
            "0      0    it 's a charming and often affecting journey .       1\n",
            "1      1                 unflinchingly bleak and desperate       0\n",
            "2      2  allows us to hope that nolan is poised to emba...      1\n",
            "3      3  the acting , costumes , music , cinematography...      1\n",
            "4      4                  it 's slow -- very , very slow .       0\n",
            "..   ...                                                ...    ...\n",
            "867  867              has all the depth of a wading pool .       0\n",
            "868  868              a movie with a real anarchic flair .       1\n",
            "869  869  a subject like this should inspire reaction in...      0\n",
            "870  870  ... is an arthritic attempt at directing by ca...      0\n",
            "871  871  looking aristocratic , luminous yet careworn i...      1\n",
            "\n",
            "[872 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. train データセットを training 用と test 用データに分割します。\n",
        "- 下記のコードセルの例では、test 用データセットのサイズを training 用データセットの 1/100 にしています。"
      ],
      "metadata": {
        "id": "P-illtJ91t5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_splitted_train, df_splitted_test = train_test_split(df_train, test_size=0.01)"
      ],
      "metadata": {
        "id": "d0BQ246I1Av_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_splitted_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrNHNbvb2zFK",
        "outputId": "f6ed0a0b-2d1a-4b05-ab6e-25ad47898c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         idx                                           sentence  label\n",
            "13894  13894                         certainly clever in spots       1\n",
            "41804  41804                          none of the happily-ever       0\n",
            "11667  11667  is a monumental achievement in practically eve...      0\n",
            "36365  36365                                 the funniest film       1\n",
            "43855  43855  serves as a painful elegy and sobering caution...      1\n",
            "...      ...                                                ...    ...\n",
            "12362  12362                                         terrorist       0\n",
            "44105  44105                                          astounds       1\n",
            "27403  27403           succeed merrily at their noble endeavor       1\n",
            "36686  36686                          with a zippy jazzy score       1\n",
            "51194  51194  a sweet , laugh-a-minute crowd pleaser that li...      1\n",
            "\n",
            "[66675 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_splitted_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qhgsmrrw23wt",
        "outputId": "18ce0094-2298-4f90-c3b9-838712162e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         idx                                           sentence  label\n",
            "44247  44247  is that i truly enjoyed most of mostly martha ...      1\n",
            "57135  57135  rife with nutty cliches and far too much dialo...      0\n",
            "20836  20836  satisfy the boom-bam crowd without a huge sacr...      1\n",
            "50268  50268                           devoid of wit and humor       0\n",
            "24155  24155  , to the adrenaline jolt of a sudden lunch rus...      1\n",
            "...      ...                                                ...    ...\n",
            "26892  26892                     dialogue , 30 seconds of plot       0\n",
            "56759  56759  an engrossing portrait of uncompromising artis...      1\n",
            "55062  55062                not a trace of humanity or empathy       0\n",
            "11289  11289  a sense of light-heartedness , that makes it a...      1\n",
            "40090  40090  we 've seen the hippie-turned-yuppie plot befo...      0\n",
            "\n",
            "[674 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. fine-tuning 前の DistilBERT のモデルをダウンロードし、SST-2 のテキストの分類ができるように fine-tuning します。\n",
        "- train データセットを分割して得られたデータセットを training に使用します。\n",
        "- validation データセットを使用して、training に使用していないテキストの分類性能が向上しているかを評価します。\n",
        "- SST-2 のテキストの token の長さは 128 以下のようなので max_length は 128 にしています (後で確認したところ最大でも token 数は 66 でした)。\n",
        "- token の長さが 128 に満たない場合は padding するようにしています。\n",
        "- training の learning rate は 1e-6 にしました。\n",
        "  - learning rate に 2e-5, 1e-5 等、一桁大きな値もセットしましたが、1e-6 にしたほうが test データの正解率は高くなりました。\n",
        "- learning rate 以外にも下記のコードの TrainingArguments, Trainer のパラメータを変えて試しました。\n",
        "  - test データを対象とした正解率は 0.8947 になりましたが、パラメータを調整したらもう少し高い値になるかもしれません。"
      ],
      "metadata": {
        "id": "Y_fEOIpnqR0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Convert from pandas to Dataset\n",
        "train_dataset = Dataset.from_pandas(df_splitted_train)\n",
        "validation_dataset = Dataset.from_pandas(df_validation)\n",
        "\n",
        "# Tokenize\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# Convert datasets to tokenized format\n",
        "tokenized_train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "tokenized_validation_dataset = validation_dataset.map(tokenize, batched=True)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./distilbert-sst2\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=1e-6,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    warmup_steps=500,\n",
        "    gradient_accumulation_steps=1,\n",
        "    fp16=True  # if available\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_validation_dataset,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save the model and tokenizer\n",
        "trainer.save_model('./fine_tuned_model_lr_1e_minus6_batch16')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291,
          "referenced_widgets": [
            "f95f62be64214f039cdedf576c80ba11",
            "ab1e87dab90641ffa5c3eaf4740e0fc7",
            "35d4192ccae84123873e841edbb9cc19",
            "f2e1c83b163c41f9978fd31302ae2402",
            "40f16f5557094baa87a0129b631f49ac",
            "d03d035225fa4388a0f3a548c11e5fbe",
            "e352291edb384ba3961f400bd96d7742",
            "d464e08858474558b2f50466c4187621",
            "235e9172d10b4401bd577c005931a1e7",
            "ddbe19f4e135479f9e5e8aeaac3f178e",
            "64c90ac2d45948678a09133c580ebac7",
            "a7b3aff8a4d94b41b065ee80e5b6d184",
            "131e4406be664b75bc3df2da779f4351",
            "b164389caa174908b0bc0fd792c00b7d",
            "d30d2da5bb404f8091f2febec4120e08",
            "a1f40db32ba64672a23af214289563c8",
            "15fc62add8fc48a7a930c6685bda51b1",
            "49eaf02c52924e33bfa5ff4ef6e70e60",
            "aaed04d967dd4136b4a468977a5bf18d",
            "2a39960c156c4082a939e00120e17274",
            "65b11f875d8842eb846f9dee4eb36713",
            "2f34c4665459416cbed060c459a5b231"
          ]
        },
        "id": "n4oxGQBijfv6",
        "outputId": "f8876616-7508-4c7e-b64c-fa31bfc68f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/66675 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f95f62be64214f039cdedf576c80ba11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7b3aff8a4d94b41b065ee80e5b6d184"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12504' max='12504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12504/12504 15:17, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.293500</td>\n",
              "      <td>0.306150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.212800</td>\n",
              "      <td>0.284342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.230500</td>\n",
              "      <td>0.284407</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. fine-tuning で得られた SST-2 の分類モデルを Google Drive に保存します。"
      ],
      "metadata": {
        "id": "sIka6Kd6n9vZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir /content/drive/MyDrive/Gifu-AI-2025-06-22/2025-05-26-v3\n",
        "cp -r /content/fine_tuned_model_lr_1e_minus6_batch16/ /content/drive/MyDrive/Gifu-AI-2025-06-22/2025-05-26-v3"
      ],
      "metadata": {
        "id": "JPAhx_w_7XJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. 上記 5. の fine-tuning で得られたモデルで test データセットを分類し、正解率を確認します。\n",
        "- 乱数を使用しているため、上記 5. までを再実行すると異なる正解率になるかと思いますが、私が確認した際には 0.8947 になりました。"
      ],
      "metadata": {
        "id": "VFw3vU4UoLO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load model and tokenizer\n",
        "fine_tuned_tokenizer = DistilBertTokenizer.from_pretrained(\"./fine_tuned_model_lr_1e_minus6_batch16\")\n",
        "fine_tuned_model = DistilBertForSequenceClassification.from_pretrained(\"./fine_tuned_model_lr_1e_minus6_batch16\")\n",
        "fine_tuned_model.eval()\n",
        "\n",
        "# Convert from pandas to Dataset\n",
        "test_dataset = Dataset.from_pandas(df_splitted_test)\n",
        "\n",
        "# Predict in batches\n",
        "preds, labels = [], []\n",
        "for example in test_dataset:\n",
        "    inputs = tokenizer(example[\"sentence\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = fine_tuned_model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    prediction = torch.argmax(logits, dim=-1).item()\n",
        "    preds.append(prediction)\n",
        "    labels.append(example[\"label\"])\n",
        "\n",
        "# Compute accuracy\n",
        "acc = accuracy_score(labels, preds)\n",
        "print(f\"Validation Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcDqjLznARNK",
        "outputId": "f62be783-290b-4362-a0b9-e635908d3b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Token 数が 128 以下になっているか気になったため、下記のコードで確認しました。\n",
        "- Token 数の最大値は 66 で、128 以下になっていました。"
      ],
      "metadata": {
        "id": "80Xg42sIpkXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Convert from pandas to Dataset\n",
        "train_check_dataset = Dataset.from_pandas(df_train)\n",
        "validation_check_dataset = Dataset.from_pandas(df_validation)\n",
        "\n",
        "# Tokenize\n",
        "def tokenize_without_truncation(batch):\n",
        "    return tokenizer(batch[\"sentence\"], truncation=False)\n",
        "\n",
        "# Convert datasets to tokenized format\n",
        "tokenized_train_dataset_without_truncation = train_check_dataset.map(tokenize_without_truncation, batched=True)\n",
        "tokenized_validation_dataset_without_truncation = validation_check_dataset.map(tokenize_without_truncation, batched=True)\n",
        "\n",
        "max_len = max(len(example[\"input_ids\"]) for example in tokenized_train_dataset_without_truncation)\n",
        "print(f\"Maximum token length (train): {max_len}\")\n",
        "\n",
        "max_len = max(len(example[\"input_ids\"]) for example in tokenized_validation_dataset_without_truncation)\n",
        "print(f\"Maximum token length (validation): {max_len}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373,
          "referenced_widgets": [
            "b66e6fc11fbd434db44a2b9369688e65",
            "e18a94c067e64478862be68b5fda1d6d",
            "b44da8b7d5ca4c6db2deb52ce99b7f3c",
            "75e7bde6e7af470fb9bc9f2339f2d918",
            "53870fc4eeae466fb37ea9f02af097e2",
            "3f71aea7913740c7921c613f5425a825",
            "ea4774784cca4cbb913467dc1c7336b8",
            "366a988eac35486e8ab79f52d424bfa4",
            "2f21031fef4641d4b4ab26ead209b2cb",
            "3eb84bd8ba9d4662909dad3da89281f8",
            "cb57b96c477449dc81f32a1884266ea9",
            "5a9c60e08f6f4a428a35729b09ec27a4",
            "3da05ca9e2fb4f8392da279132957069",
            "9e95e06c506e4a36b7c7d167dcba4dd5",
            "88e8ddfcb53040e8b16b4a4f75d5c21a",
            "40b2cda1eb1d4dd88133fc73a71d65de",
            "b32889c74bde499abf02a1f7d5ad553b",
            "a48e9574fa344e8fbdecdaaade2f181d",
            "e01c8d5541354b95ade32cbeeeeae355",
            "2d252a0e1f2742558bdf8c30adef1ec4",
            "62f8bc0d4595451cbd5da1b253f592be",
            "7a57e032d71c450fb6b3dd4f7a3d1fdb",
            "b2a259f87ab8456292b25a89176c376a",
            "e3f774366e124ca1877c4c09b05e4b46",
            "786ef10e12bf4176865ea4dff4e9569f",
            "30547eb37335465a988481e428a4bef8",
            "0f9b6175259c459eb4f62d1dd439578c",
            "1d89eddd46474d398d1b080e3307821b",
            "adac49732121493eb8c6ff3817b09b14",
            "be182d4c9ad64491aa7c01fcbd380818",
            "ea859b9e34cd4f1ea0080df505f3ed49",
            "ccf707c80a6c4f64afa713d90fe1b489",
            "65d62494473b4dd1a32361440ea551c4",
            "5f75f4ba9c604c6f9e965fb6c5436b78",
            "2ed2b409a68e4154a705be5172435a15",
            "9d0e530c12654788a4018f01e71bcb35",
            "08918e20ef634a8f89b29e2a2be6bc78",
            "7f23e80591dd4183ae215f8af915d7ab",
            "05587be3c8484be3a3b94a70b413bdd1",
            "fc374e0ff6e547e49fa4b95205100e4c",
            "29fc1e196abe48d49b3906572f99c232",
            "0c4232fa2ec348f18de218884973038b",
            "7d551c4c7fc24b5fa573a5cf9308f205",
            "9f5151a3f507456c82be48867e887c93",
            "aa9f2c0f9ea943e886e1c9c1011d0075",
            "5863d34e045b42cbb396ba8ca5f62c38",
            "d6d490548cc9474eadba553b85d58bad",
            "ce627bd77d07429c999f1bb9ab2b02c7",
            "780311a1d1c4403882314370f29ea17f",
            "403a503557b644dab226c2a749a376f9",
            "2c33f132bbfd4691a68ef33583f36add",
            "55ce647952cd472599a362d49cd42b1c",
            "593c6548d85140daa99eefa9b0abf7f2",
            "3add1d5231a74a0a9a4cdc2196599e60",
            "1c451c8be3d7439db56db1f32ef49867",
            "5348f53029be400b865ca43b4eb1c602",
            "38ee94ab62b84a6db4fb44e1781b7050",
            "9a3760a206c44a9fb38fa2117db8db60",
            "8342d013d1f64d8fbda7f622e13fc8ef",
            "80c7fb6be0e7474b82b120cf28fdced1",
            "9bd70e9a67234bcdbacbed5f3a1724da",
            "e84b8d46a81e478ba5b8fef1f9111f33",
            "e0332c6ab5e949e782203949ce0f2d92",
            "1173a8f62d09495d9919992d152f9bc8",
            "e346d450157541038773d5c6780353ad",
            "d96b23319ee94390a058a45c04089794",
            "827b67ef92554af5ad510c7e30a5e9db",
            "3e9ad79a1f534480bbe63c0d0712a369",
            "aeab778c9ca644c1ad924b145e287bad",
            "bf859268db0140cf9c083efc2031ca55",
            "28a5bc40b39c45a6b8598df7dc72b9d9",
            "01819a35257348eb9cf50e0418f61d84",
            "16a4bc9adaf14e969108fbe6b0ffe825",
            "5159354b94d545a3b90224f79b8092bb",
            "e8cc74a386be4625a65029616a377a78",
            "2904a3f62d6a4298a99c4c224c4e6ed6",
            "7bb0984c3358464e9160a3e5ea52909e"
          ]
        },
        "id": "ug8P7L3uiNB0",
        "outputId": "963c6266-c5f6-47a5-9415-5473e4f48876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b66e6fc11fbd434db44a2b9369688e65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a9c60e08f6f4a428a35729b09ec27a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2a259f87ab8456292b25a89176c376a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f75f4ba9c604c6f9e965fb6c5436b78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa9f2c0f9ea943e886e1c9c1011d0075"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5348f53029be400b865ca43b4eb1c602"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "827b67ef92554af5ad510c7e30a5e9db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum token length (train): 66\n",
            "Maximum token length (validation): 55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIyBqi4bDihS",
        "outputId": "e770c07d-254f-4f75-8d12-7e1f3d0387c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['idx', 'sentence', 'label', '__index_level_0__'],\n",
            "    num_rows: 674\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(validation_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZJQKoDxDlBO",
        "outputId": "c506dd04-7943-4d39-ede8-05edc3604e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['idx', 'sentence', 'label'],\n",
            "    num_rows: 872\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77FCwkU3pvN_",
        "outputId": "5de46a52-f9e4-45d6-8b69-f1f9cec86b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['idx', 'sentence', 'label', '__index_level_0__'],\n",
            "    num_rows: 66675\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQRX6CpJEmT6",
        "outputId": "5fc1c24c-87ac-42af-f47d-b653c8b3cba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghu1gJoPEr3e",
        "outputId": "427e19e8-c129-4b5a-d192-fc87bbb3ad7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n"
          ]
        }
      ]
    }
  ]
}